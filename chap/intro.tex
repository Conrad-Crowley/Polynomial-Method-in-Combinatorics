\chapter{Introduction}
The following is a short exposition of polynomial methods in combinatorics. Polynomial methods are a collection of techniques which use polynomial interpolation and rigidity properties
of polynomials to control the size of collections of objects with a certain structure. 
The first example of this technique was presented in the 1990s in \cite{alon1999combinatorial},  which we examine in detail
in Chapter \ref{chap:alon}. The modern conception of the polynomial method was pioneered by Dvir in 2008 (See \cite{2008DVIR}), where he produced a remarkably short resolution to the finite field analogue of the Kakeya Conjecture which provided
a new framework and enthusiasm for the polynomial method in combinatorial problems. We will explore this proof in the next chapter. 

The most striking feature of the following proofs is that they leverage certain properties of polynomials in problems which on the surface appear not to have anything to 
do with polynomials. Generally, extremal configurations of these problems tend to admit a lot of algebraic structure and this is exactly what these methods exploit using polynomials. 


\section{Why Polynomials?}
It is perhaps wise to discuss here what features of polynomials make them particularly powerful when dealing with problems in Combinatorics. 
Polynomials are perhaps some of the simplest of mathematical objects, as once we define a field they are simply a combination of the addition and multiplication operation between elements. 
It is not immediately obvious why such simple objects may prove to be so useful.

There are two key properties of polynomials that this collection of methods exploit.
Firstly, we use the fact that there are roughly $\sim D^n$ coefficients of a polynomial in $n$ variables of degree at most $D$ (See Lemma \ref{lem:paramcounting}).  This is utilised 
in an essential manner when we try to find polynomials that contain objects in their zero set. We can contain a set of size $M$ in the zero set of
a polynomial with degree at most $O(M^{1/n}$). In other words, we have a lot of flexibility in choosing a polynomial.
Secondly, and in sharp contrast, polynomials behave extremely rigidly when restricted to lines. We mean by this that the zero set of a polynomial of degree $D$ can intersect 
a line in at most $D$ points if the line is not contained within said zero set. The gap between this flexibility of choosing a polynomial and rigidity of restricting to lines provides
us with a surprisingly powerful technique. 

Another striking thing about the method is the non-constructive manner in which the polynomials are usually used. 
We often cannot explicitly find a satisfactory polynomial to use for our purposes, instead we opt to use arguments from Linear Algebra to establish the
existence of a polynomial with such properties. This is reminiscent of other methods in combinatorics, such as the Probabilistic Method or the Topological Method (See \cite{ALON2003} for a survey of these methods). 

\section{Notation}
We introduce some convenient notation here. We write that $A \lesssim_n B$ to mean that there exists some constant
$C(n)$ which depends on $n$ such that $A \leq C(n) B$. Further, we write that $A \sim_n B$ if $A \lesssim_n B$ and $B \lesssim_n A$.

We write $\text{Poly}_D (\KK^n)$ to represent the space of polynomials in $n$ variables with coefficients in a field $\KK$ and degree at most $D$.

The indicator function $\OO$ is defined on logical statements $X$ as follows:
\[
    \OO[X] = 
  \begin{cases}
      1 & \text{if X is true}, \\
      0 & \text{if X is false}.
  \end{cases}  
\]
For any function $f : \RR^n \to \RR$ let us denote the zero set of $f$ by $Z(f) = \{x \in \RR^n \ | \ f(x) = 0\}.$
We borrow from Computer Science the big O notation. For functions $f,g : \NN^+ \to \RR$ we write:
\begin{align*}
    f(N) &= O(g(N)) \iff \exists N_0, M \in \NN \text{ such that } f(n) \leq Mg(n) \forall n > N_0 \\
    f(N) &= \Omega(g(n)) \iff g(N) = O(f(N)).
\end{align*}
These can be thought of as asymptotic upper and lower bounds respectively.